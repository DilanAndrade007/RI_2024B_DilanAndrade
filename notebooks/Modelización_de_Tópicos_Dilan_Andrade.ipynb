{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recuperacion de la informacion\n",
        "### Nombre: Dilan Andrade\n",
        "### Tema: Modelización de Tópicos\n",
        "### Fecha: 14-01-2025"
      ],
      "metadata": {
        "id": "4zfHvngAPbgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga y Exploración Inicial de Datos\n",
        "\n",
        "1. **Carga del CSV**: Utiliza `pandas` para cargar un archivo CSV que contiene los datos del podcast.\n",
        "2. **Inspección Inicial**: Muestra las primeras filas del archivo con `data.head()`, lo que ayuda a verificar que los datos se han cargado correctamente.\n"
      ],
      "metadata": {
        "id": "9-2MazEWFWJR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDPTduZK__8K",
        "outputId": "e780e542-88c9-4f36-a6be-40355aace9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id            guest                    title  \\\n",
            "0   1      Max Tegmark                 Life 3.0   \n",
            "1   2    Christof Koch            Consciousness   \n",
            "2   3    Steven Pinker  AI in the Age of Reason   \n",
            "3   4    Yoshua Bengio            Deep Learning   \n",
            "4   5  Vladimir Vapnik     Statistical Learning   \n",
            "\n",
            "                                                text  \n",
            "0  As part of MIT course 6S099, Artificial Genera...  \n",
            "1  As part of MIT course 6S099 on artificial gene...  \n",
            "2  You've studied the human mind, cognition, lang...  \n",
            "3  What difference between biological neural netw...  \n",
            "4  The following is a conversation with Vladimir ...  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "# Cargar el archivo CSV\n",
        "file_path = '/content/podcastdata_dataset.csv'  # Cambia la ruta si es necesario\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Inspeccionar las primeras filas del archivo\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar palabras en la columna 'text'\n",
        "# Agregar una nueva columna llamada 'word_count' con el conteo de palabras\n",
        "data['word_count'] = data['text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Mostrar los episodios con su conteo de palabras\n",
        "for index, row in data.iterrows():\n",
        "    print(f\"Episodio {row['id']}: {row['word_count']} palabras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-gZxLmvOACDi",
        "outputId": "ee970aef-850f-482f-a580-100126ccc51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episodio 1: 13424 palabras\n",
            "Episodio 2: 10217 palabras\n",
            "Episodio 3: 5989 palabras\n",
            "Episodio 4: 5993 palabras\n",
            "Episodio 5: 6374 palabras\n",
            "Episodio 6: 11219 palabras\n",
            "Episodio 7: 17372 palabras\n",
            "Episodio 8: 5473 palabras\n",
            "Episodio 9: 12453 palabras\n",
            "Episodio 10: 7374 palabras\n",
            "Episodio 11: 10667 palabras\n",
            "Episodio 12: 10608 palabras\n",
            "Episodio 13: 9799 palabras\n",
            "Episodio 14: 10702 palabras\n",
            "Episodio 14: 10702 palabras\n",
            "Episodio 15: 10277 palabras\n",
            "Episodio 16: 13059 palabras\n",
            "Episodio 17: 16251 palabras\n",
            "Episodio 18: 5118 palabras\n",
            "Episodio 19: 11669 palabras\n",
            "Episodio 20: 17893 palabras\n",
            "Episodio 21: 13054 palabras\n",
            "Episodio 22: 11890 palabras\n",
            "Episodio 23: 11960 palabras\n",
            "Episodio 24: 9726 palabras\n",
            "Episodio 25: 24567 palabras\n",
            "Episodio 26: 6008 palabras\n",
            "Episodio 27: 11906 palabras\n",
            "Episodio 28: 6834 palabras\n",
            "Episodio 29: 18645 palabras\n",
            "Episodio 30: 8975 palabras\n",
            "Episodio 31: 21348 palabras\n",
            "Episodio 32: 8787 palabras\n",
            "Episodio 33: 10864 palabras\n",
            "Episodio 34: 8132 palabras\n",
            "Episodio 35: 16772 palabras\n",
            "Episodio 36: 12577 palabras\n",
            "Episodio 37: 8693 palabras\n",
            "Episodio 38: 17681 palabras\n",
            "Episodio 39: 4928 palabras\n",
            "Episodio 40: 12279 palabras\n",
            "Episodio 41: 8318 palabras\n",
            "Episodio 42: 10026 palabras\n",
            "Episodio 43: 15852 palabras\n",
            "Episodio 44: 23584 palabras\n",
            "Episodio 45: 9089 palabras\n",
            "Episodio 46: 8670 palabras\n",
            "Episodio 47: 15842 palabras\n",
            "Episodio 48: 14011 palabras\n",
            "Episodio 49: 5643 palabras\n",
            "Episodio 50: 17955 palabras\n",
            "Episodio 51: 7741 palabras\n",
            "Episodio 52: 6648 palabras\n",
            "Episodio 53: 4956 palabras\n",
            "Episodio 54: 14098 palabras\n",
            "Episodio 55: 14452 palabras\n",
            "Episodio 56: 10616 palabras\n",
            "Episodio 57: 17832 palabras\n",
            "Episodio 58: 9409 palabras\n",
            "Episodio 59: 13804 palabras\n",
            "Episodio 60: 15344 palabras\n",
            "Episodio 61: 15756 palabras\n",
            "Episodio 62: 14289 palabras\n",
            "Episodio 63: 12998 palabras\n",
            "Episodio 64: 11573 palabras\n",
            "Episodio 65: 10641 palabras\n",
            "Episodio 66: 15856 palabras\n",
            "Episodio 67: 8875 palabras\n",
            "Episodio 68: 14529 palabras\n",
            "Episodio 69: 15916 palabras\n",
            "Episodio 70: 14880 palabras\n",
            "Episodio 71: 12310 palabras\n",
            "Episodio 72: 15162 palabras\n",
            "Episodio 73: 15975 palabras\n",
            "Episodio 74: 21229 palabras\n",
            "Episodio 75: 15947 palabras\n",
            "Episodio 76: 9386 palabras\n",
            "Episodio 77: 11018 palabras\n",
            "Episodio 78: 9493 palabras\n",
            "Episodio 79: 9731 palabras\n",
            "Episodio 80: 15457 palabras\n",
            "Episodio 81: 16416 palabras\n",
            "Episodio 82: 6478 palabras\n",
            "Episodio 83: 18076 palabras\n",
            "Episodio 84: 14018 palabras\n",
            "Episodio 85: 14522 palabras\n",
            "Episodio 86: 17477 palabras\n",
            "Episodio 87: 10303 palabras\n",
            "Episodio 88: 27377 palabras\n",
            "Episodio 89: 33770 palabras\n",
            "Episodio 90: 15308 palabras\n",
            "Episodio 91: 7650 palabras\n",
            "Episodio 92: 18639 palabras\n",
            "Episodio 93: 10815 palabras\n",
            "Episodio 94: 16293 palabras\n",
            "Episodio 95: 19376 palabras\n",
            "Episodio 96: 8981 palabras\n",
            "Episodio 97: 15678 palabras\n",
            "Episodio 98: 11606 palabras\n",
            "Episodio 99: 12699 palabras\n",
            "Episodio 101: 31589 palabras\n",
            "Episodio 102: 13228 palabras\n",
            "Episodio 103: 39011 palabras\n",
            "Episodio 104: 18087 palabras\n",
            "Episodio 105: 10451 palabras\n",
            "Episodio 106: 17393 palabras\n",
            "Episodio 107: 10642 palabras\n",
            "Episodio 108: 17258 palabras\n",
            "Episodio 109: 17554 palabras\n",
            "Episodio 110: 14144 palabras\n",
            "Episodio 111: 15667 palabras\n",
            "Episodio 112: 17967 palabras\n",
            "Episodio 113: 24109 palabras\n",
            "Episodio 114: 26267 palabras\n",
            "Episodio 115: 19044 palabras\n",
            "Episodio 116: 17975 palabras\n",
            "Episodio 117: 23286 palabras\n",
            "Episodio 118: 24645 palabras\n",
            "Episodio 119: 18379 palabras\n",
            "Episodio 120: 22394 palabras\n",
            "Episodio 121: 29646 palabras\n",
            "Episodio 122: 48906 palabras\n",
            "Episodio 123: 21171 palabras\n",
            "Episodio 124: 45903 palabras\n",
            "Episodio 125: 45364 palabras\n",
            "Episodio 126: 12885 palabras\n",
            "Episodio 127: 13645 palabras\n",
            "Episodio 128: 35060 palabras\n",
            "Episodio 129: 19648 palabras\n",
            "Episodio 130: 18819 palabras\n",
            "Episodio 131: 29399 palabras\n",
            "Episodio 132: 31272 palabras\n",
            "Episodio 133: 23879 palabras\n",
            "Episodio 134: 27221 palabras\n",
            "Episodio 135: 27400 palabras\n",
            "Episodio 136: 33932 palabras\n",
            "Episodio 137: 24739 palabras\n",
            "Episodio 138: 27433 palabras\n",
            "Episodio 139: 26864 palabras\n",
            "Episodio 140: 20581 palabras\n",
            "Episodio 141: 17964 palabras\n",
            "Episodio 142: 20371 palabras\n",
            "Episodio 143: 30182 palabras\n",
            "Episodio 144: 20517 palabras\n",
            "Episodio 145: 35135 palabras\n",
            "Episodio 146: 22220 palabras\n",
            "Episodio 147: 24391 palabras\n",
            "Episodio 148: 23241 palabras\n",
            "Episodio 149: 27261 palabras\n",
            "Episodio 150: 30330 palabras\n",
            "Episodio 151: 15240 palabras\n",
            "Episodio 152: 11385 palabras\n",
            "Episodio 153: 17516 palabras\n",
            "Episodio 154: 25867 palabras\n",
            "Episodio 155: 29848 palabras\n",
            "Episodio 156: 26838 palabras\n",
            "Episodio 157: 14061 palabras\n",
            "Episodio 158: 13451 palabras\n",
            "Episodio 159: 17509 palabras\n",
            "Episodio 160: 33451 palabras\n",
            "Episodio 161: 25499 palabras\n",
            "Episodio 162: 27132 palabras\n",
            "Episodio 163: 25043 palabras\n",
            "Episodio 164: 30198 palabras\n",
            "Episodio 165: 21467 palabras\n",
            "Episodio 166: 35297 palabras\n",
            "Episodio 167: 33060 palabras\n",
            "Episodio 168: 16440 palabras\n",
            "Episodio 169: 34727 palabras\n",
            "Episodio 170: 13746 palabras\n",
            "Episodio 171: 34033 palabras\n",
            "Episodio 172: 24131 palabras\n",
            "Episodio 173: 22696 palabras\n",
            "Episodio 174: 21447 palabras\n",
            "Episodio 175: 21053 palabras\n",
            "Episodio 176: 37199 palabras\n",
            "Episodio 177: 18471 palabras\n",
            "Episodio 178: 50109 palabras\n",
            "Episodio 179: 26659 palabras\n",
            "Episodio 180: 22038 palabras\n",
            "Episodio 181: 29455 palabras\n",
            "Episodio 182: 32537 palabras\n",
            "Episodio 183: 24650 palabras\n",
            "Episodio 184: 18780 palabras\n",
            "Episodio 185: 29729 palabras\n",
            "Episodio 186: 23825 palabras\n",
            "Episodio 187: 18307 palabras\n",
            "Episodio 188: 30736 palabras\n",
            "Episodio 189: 17456 palabras\n",
            "Episodio 190: 29760 palabras\n",
            "Episodio 191: 41584 palabras\n",
            "Episodio 192: 59475 palabras\n",
            "Episodio 193: 29525 palabras\n",
            "Episodio 194: 31006 palabras\n",
            "Episodio 195: 18766 palabras\n",
            "Episodio 196: 19146 palabras\n",
            "Episodio 197: 18731 palabras\n",
            "Episodio 198: 21020 palabras\n",
            "Episodio 199: 23444 palabras\n",
            "Episodio 200: 28941 palabras\n",
            "Episodio 201: 23977 palabras\n",
            "Episodio 202: 24315 palabras\n",
            "Episodio 203: 18629 palabras\n",
            "Episodio 204: 23707 palabras\n",
            "Episodio 205: 37679 palabras\n",
            "Episodio 206: 28576 palabras\n",
            "Episodio 208: 25732 palabras\n",
            "Episodio 209: 30092 palabras\n",
            "Episodio 210: 25864 palabras\n",
            "Episodio 211: 17150 palabras\n",
            "Episodio 212: 31697 palabras\n",
            "Episodio 213: 20387 palabras\n",
            "Episodio 214: 15390 palabras\n",
            "Episodio 215: 27355 palabras\n",
            "Episodio 216: 34207 palabras\n",
            "Episodio 217: 21938 palabras\n",
            "Episodio 218: 18285 palabras\n",
            "Episodio 219: 18633 palabras\n",
            "Episodio 220: 28456 palabras\n",
            "Episodio 221: 25888 palabras\n",
            "Episodio 222: 18789 palabras\n",
            "Episodio 223: 37509 palabras\n",
            "Episodio 224: 36502 palabras\n",
            "Episodio 225: 28319 palabras\n",
            "Episodio 226: 14648 palabras\n",
            "Episodio 227: 25917 palabras\n",
            "Episodio 228: 13134 palabras\n",
            "Episodio 229: 21000 palabras\n",
            "Episodio 230: 44613 palabras\n",
            "Episodio 231: 27355 palabras\n",
            "Episodio 232: 16728 palabras\n",
            "Episodio 233: 10769 palabras\n",
            "Episodio 234: 39990 palabras\n",
            "Episodio 235: 17632 palabras\n",
            "Episodio 236: 25550 palabras\n",
            "Episodio 237: 32110 palabras\n",
            "Episodio 238: 13584 palabras\n",
            "Episodio 239: 23649 palabras\n",
            "Episodio 240: 21676 palabras\n",
            "Episodio 241: 34606 palabras\n",
            "Episodio 242: 28083 palabras\n",
            "Episodio 243: 28924 palabras\n",
            "Episodio 244: 27394 palabras\n",
            "Episodio 245: 9629 palabras\n",
            "Episodio 246: 22470 palabras\n",
            "Episodio 247: 45109 palabras\n",
            "Episodio 248: 21087 palabras\n",
            "Episodio 249: 10495 palabras\n",
            "Episodio 250: 30100 palabras\n",
            "Episodio 251: 12335 palabras\n",
            "Episodio 252: 22141 palabras\n",
            "Episodio 253: 35098 palabras\n",
            "Episodio 254: 22531 palabras\n",
            "Episodio 255: 20175 palabras\n",
            "Episodio 256: 28837 palabras\n",
            "Episodio 257: 44312 palabras\n",
            "Episodio 258: 27053 palabras\n",
            "Episodio 259: 20326 palabras\n",
            "Episodio 260: 31351 palabras\n",
            "Episodio 261: 25238 palabras\n",
            "Episodio 262: 15265 palabras\n",
            "Episodio 263: 19219 palabras\n",
            "Episodio 264: 31746 palabras\n",
            "Episodio 265: 19971 palabras\n",
            "Episodio 266: 18300 palabras\n",
            "Episodio 267: 20696 palabras\n",
            "Episodio 269: 41740 palabras\n",
            "Episodio 270: 20979 palabras\n",
            "Episodio 271: 19050 palabras\n",
            "Episodio 272: 39062 palabras\n",
            "Episodio 273: 28685 palabras\n",
            "Episodio 274: 30637 palabras\n",
            "Episodio 275: 15304 palabras\n",
            "Episodio 276: 34869 palabras\n",
            "Episodio 277: 36931 palabras\n",
            "Episodio 278: 22231 palabras\n",
            "Episodio 279: 43876 palabras\n",
            "Episodio 280: 10843 palabras\n",
            "Episodio 281: 21712 palabras\n",
            "Episodio 284: 44615 palabras\n",
            "Episodio 285: 30243 palabras\n",
            "Episodio 286: 17938 palabras\n",
            "Episodio 288: 40695 palabras\n",
            "Episodio 289: 21917 palabras\n",
            "Episodio 290: 22803 palabras\n",
            "Episodio 292: 42925 palabras\n",
            "Episodio 293: 30533 palabras\n",
            "Episodio 294: 28493 palabras\n",
            "Episodio 295: 24873 palabras\n",
            "Episodio 296: 24993 palabras\n",
            "Episodio 297: 27571 palabras\n",
            "Episodio 298: 19943 palabras\n",
            "Episodio 299: 23448 palabras\n",
            "Episodio 300: 16887 palabras\n",
            "Episodio 301: 31340 palabras\n",
            "Episodio 302: 21826 palabras\n",
            "Episodio 303: 35054 palabras\n",
            "Episodio 304: 19223 palabras\n",
            "Episodio 305: 19910 palabras\n",
            "Episodio 306: 20334 palabras\n",
            "Episodio 307: 28214 palabras\n",
            "Episodio 308: 27520 palabras\n",
            "Episodio 309: 57667 palabras\n",
            "Episodio 310: 38316 palabras\n",
            "Episodio 311: 35145 palabras\n",
            "Episodio 312: 32437 palabras\n",
            "Episodio 313: 29793 palabras\n",
            "Episodio 314: 37866 palabras\n",
            "Episodio 315: 23159 palabras\n",
            "Episodio 316: 5069 palabras\n",
            "Episodio 317: 28369 palabras\n",
            "Episodio 318: 36564 palabras\n",
            "Episodio 319: 28794 palabras\n",
            "Episodio 320: 19902 palabras\n",
            "Episodio 321: 12807 palabras\n",
            "Episodio 322: 26034 palabras\n",
            "Episodio 323: 25255 palabras\n",
            "Episodio 324: 29911 palabras\n",
            "Episodio 325: 33714 palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conteo de Palabras\n",
        "\n",
        "1. **Contar Palabras**: Calcula el número de palabras en cada episodio utilizando el método `apply()`, que aplica una función lambda para dividir el texto en palabras y contar su longitud.\n",
        "2. **Mostrar Resultados**: Itera sobre el DataFrame y muestra el número de palabras en cada episodio.\n"
      ],
      "metadata": {
        "id": "tg26ew6tG-pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar palabras en la columna 'text'\n",
        "# Agregar una nueva columna llamada 'word_count' con el conteo de palabras\n",
        "conteo = data[data['id'] == 1]['text'].apply(lambda x: len(str(x).split())).values[0]\n",
        "\n",
        "\n",
        "# Mostrar los episodios con su conteo de palabras\n",
        "print(f\"Episodio 1 tiene {conteo} palabras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGhirvkWACHp",
        "outputId": "9cb343a2-4604-4404-9aef-d887a3100fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episodio 1 tiene 13424 palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conteo de Oraciones\n",
        "\n",
        "1. **Contar Oraciones**: Utiliza expresiones regulares con `re.split()` para dividir el texto en oraciones, separando por los delimitadores de puntuación comunes (`.`, `!`, `?`). Luego, calcula el número de oraciones.\n",
        "2. **Mostrar Resultados**: Itera sobre el DataFrame y muestra el número de oraciones en cada episodio.\n"
      ],
      "metadata": {
        "id": "5B9XXbKCHD2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Contar oraciones en la columna 'text'\n",
        "data['sentence_count'] = data['text'].apply(lambda x: len(re.split(r'[.!?]+', str(x).strip())) - 1)\n",
        "\n",
        "# Mostrar los episodios con su conteo de oraciones\n",
        "for index, row in data.iterrows():\n",
        "    print(f\"Episodio {row['id']}: {row['sentence_count']} oraciones\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BrNAGdIJACKI",
        "outputId": "a0328b51-9de1-436e-949c-984897651484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episodio 1: 744 oraciones\n",
            "Episodio 2: 606 oraciones\n",
            "Episodio 3: 335 oraciones\n",
            "Episodio 4: 359 oraciones\n",
            "Episodio 5: 563 oraciones\n",
            "Episodio 6: 514 oraciones\n",
            "Episodio 7: 1395 oraciones\n",
            "Episodio 8: 324 oraciones\n",
            "Episodio 9: 706 oraciones\n",
            "Episodio 10: 434 oraciones\n",
            "Episodio 11: 496 oraciones\n",
            "Episodio 12: 692 oraciones\n",
            "Episodio 13: 669 oraciones\n",
            "Episodio 14: 549 oraciones\n",
            "Episodio 14: 549 oraciones\n",
            "Episodio 15: 688 oraciones\n",
            "Episodio 16: 795 oraciones\n",
            "Episodio 17: 871 oraciones\n",
            "Episodio 18: 278 oraciones\n",
            "Episodio 19: 564 oraciones\n",
            "Episodio 20: 1062 oraciones\n",
            "Episodio 21: 876 oraciones\n",
            "Episodio 22: 779 oraciones\n",
            "Episodio 23: 634 oraciones\n",
            "Episodio 24: 625 oraciones\n",
            "Episodio 25: 1770 oraciones\n",
            "Episodio 26: 383 oraciones\n",
            "Episodio 27: 601 oraciones\n",
            "Episodio 28: 362 oraciones\n",
            "Episodio 29: 1275 oraciones\n",
            "Episodio 30: 426 oraciones\n",
            "Episodio 31: 1920 oraciones\n",
            "Episodio 32: 515 oraciones\n",
            "Episodio 33: 603 oraciones\n",
            "Episodio 34: 723 oraciones\n",
            "Episodio 35: 1022 oraciones\n",
            "Episodio 36: 831 oraciones\n",
            "Episodio 37: 577 oraciones\n",
            "Episodio 38: 1167 oraciones\n",
            "Episodio 39: 275 oraciones\n",
            "Episodio 40: 638 oraciones\n",
            "Episodio 41: 657 oraciones\n",
            "Episodio 42: 601 oraciones\n",
            "Episodio 43: 968 oraciones\n",
            "Episodio 44: 1491 oraciones\n",
            "Episodio 45: 750 oraciones\n",
            "Episodio 46: 589 oraciones\n",
            "Episodio 47: 983 oraciones\n",
            "Episodio 48: 1061 oraciones\n",
            "Episodio 49: 371 oraciones\n",
            "Episodio 50: 660 oraciones\n",
            "Episodio 51: 527 oraciones\n",
            "Episodio 52: 533 oraciones\n",
            "Episodio 53: 327 oraciones\n",
            "Episodio 54: 829 oraciones\n",
            "Episodio 55: 904 oraciones\n",
            "Episodio 56: 1000 oraciones\n",
            "Episodio 57: 1061 oraciones\n",
            "Episodio 58: 570 oraciones\n",
            "Episodio 59: 977 oraciones\n",
            "Episodio 60: 998 oraciones\n",
            "Episodio 61: 1009 oraciones\n",
            "Episodio 62: 880 oraciones\n",
            "Episodio 63: 789 oraciones\n",
            "Episodio 64: 669 oraciones\n",
            "Episodio 65: 687 oraciones\n",
            "Episodio 66: 1085 oraciones\n",
            "Episodio 67: 547 oraciones\n",
            "Episodio 68: 809 oraciones\n",
            "Episodio 69: 968 oraciones\n",
            "Episodio 70: 1199 oraciones\n",
            "Episodio 71: 1176 oraciones\n",
            "Episodio 72: 789 oraciones\n",
            "Episodio 73: 528 oraciones\n",
            "Episodio 74: 1331 oraciones\n",
            "Episodio 75: 1077 oraciones\n",
            "Episodio 76: 564 oraciones\n",
            "Episodio 77: 672 oraciones\n",
            "Episodio 78: 468 oraciones\n",
            "Episodio 79: 670 oraciones\n",
            "Episodio 80: 143 oraciones\n",
            "Episodio 81: 914 oraciones\n",
            "Episodio 82: 415 oraciones\n",
            "Episodio 83: 865 oraciones\n",
            "Episodio 84: 759 oraciones\n",
            "Episodio 85: 1076 oraciones\n",
            "Episodio 86: 830 oraciones\n",
            "Episodio 87: 637 oraciones\n",
            "Episodio 88: 1945 oraciones\n",
            "Episodio 89: 1945 oraciones\n",
            "Episodio 90: 1006 oraciones\n",
            "Episodio 91: 480 oraciones\n",
            "Episodio 92: 1096 oraciones\n",
            "Episodio 93: 509 oraciones\n",
            "Episodio 94: 1086 oraciones\n",
            "Episodio 95: 1195 oraciones\n",
            "Episodio 96: 593 oraciones\n",
            "Episodio 97: 827 oraciones\n",
            "Episodio 98: 781 oraciones\n",
            "Episodio 99: 661 oraciones\n",
            "Episodio 101: 2092 oraciones\n",
            "Episodio 102: 900 oraciones\n",
            "Episodio 103: 2234 oraciones\n",
            "Episodio 104: 1102 oraciones\n",
            "Episodio 105: 664 oraciones\n",
            "Episodio 106: 970 oraciones\n",
            "Episodio 107: 489 oraciones\n",
            "Episodio 108: 790 oraciones\n",
            "Episodio 109: 1136 oraciones\n",
            "Episodio 110: 880 oraciones\n",
            "Episodio 111: 886 oraciones\n",
            "Episodio 112: 305 oraciones\n",
            "Episodio 113: 1587 oraciones\n",
            "Episodio 114: 1663 oraciones\n",
            "Episodio 115: 1509 oraciones\n",
            "Episodio 116: 1215 oraciones\n",
            "Episodio 117: 1551 oraciones\n",
            "Episodio 118: 1310 oraciones\n",
            "Episodio 119: 1109 oraciones\n",
            "Episodio 120: 1494 oraciones\n",
            "Episodio 121: 1751 oraciones\n",
            "Episodio 122: 2967 oraciones\n",
            "Episodio 123: 1514 oraciones\n",
            "Episodio 124: 2785 oraciones\n",
            "Episodio 125: 2887 oraciones\n",
            "Episodio 126: 980 oraciones\n",
            "Episodio 127: 1084 oraciones\n",
            "Episodio 128: 2925 oraciones\n",
            "Episodio 129: 1242 oraciones\n",
            "Episodio 130: 1119 oraciones\n",
            "Episodio 131: 1964 oraciones\n",
            "Episodio 132: 2764 oraciones\n",
            "Episodio 133: 1416 oraciones\n",
            "Episodio 134: 2207 oraciones\n",
            "Episodio 135: 1930 oraciones\n",
            "Episodio 136: 1919 oraciones\n",
            "Episodio 137: 1346 oraciones\n",
            "Episodio 138: 2120 oraciones\n",
            "Episodio 139: 1534 oraciones\n",
            "Episodio 140: 1309 oraciones\n",
            "Episodio 141: 1071 oraciones\n",
            "Episodio 142: 1374 oraciones\n",
            "Episodio 143: 2018 oraciones\n",
            "Episodio 144: 1398 oraciones\n",
            "Episodio 145: 1784 oraciones\n",
            "Episodio 146: 1308 oraciones\n",
            "Episodio 147: 1308 oraciones\n",
            "Episodio 148: 1912 oraciones\n",
            "Episodio 149: 2053 oraciones\n",
            "Episodio 150: 2349 oraciones\n",
            "Episodio 151: 1072 oraciones\n",
            "Episodio 152: 865 oraciones\n",
            "Episodio 153: 1110 oraciones\n",
            "Episodio 154: 1687 oraciones\n",
            "Episodio 155: 1703 oraciones\n",
            "Episodio 156: 2069 oraciones\n",
            "Episodio 157: 863 oraciones\n",
            "Episodio 158: 724 oraciones\n",
            "Episodio 159: 1203 oraciones\n",
            "Episodio 160: 2292 oraciones\n",
            "Episodio 161: 1825 oraciones\n",
            "Episodio 162: 2193 oraciones\n",
            "Episodio 163: 1661 oraciones\n",
            "Episodio 164: 1899 oraciones\n",
            "Episodio 165: 1436 oraciones\n",
            "Episodio 166: 2378 oraciones\n",
            "Episodio 167: 2398 oraciones\n",
            "Episodio 168: 1154 oraciones\n",
            "Episodio 169: 2488 oraciones\n",
            "Episodio 170: 725 oraciones\n",
            "Episodio 171: 951 oraciones\n",
            "Episodio 172: 1758 oraciones\n",
            "Episodio 173: 1494 oraciones\n",
            "Episodio 174: 1634 oraciones\n",
            "Episodio 175: 1666 oraciones\n",
            "Episodio 176: 2284 oraciones\n",
            "Episodio 177: 1139 oraciones\n",
            "Episodio 178: 4255 oraciones\n",
            "Episodio 179: 1715 oraciones\n",
            "Episodio 180: 1474 oraciones\n",
            "Episodio 181: 1546 oraciones\n",
            "Episodio 182: 2182 oraciones\n",
            "Episodio 183: 1582 oraciones\n",
            "Episodio 184: 1129 oraciones\n",
            "Episodio 185: 1536 oraciones\n",
            "Episodio 186: 1415 oraciones\n",
            "Episodio 187: 1003 oraciones\n",
            "Episodio 188: 1614 oraciones\n",
            "Episodio 189: 1187 oraciones\n",
            "Episodio 190: 1893 oraciones\n",
            "Episodio 191: 1983 oraciones\n",
            "Episodio 192: 3883 oraciones\n",
            "Episodio 193: 1387 oraciones\n",
            "Episodio 194: 1679 oraciones\n",
            "Episodio 195: 1457 oraciones\n",
            "Episodio 196: 1734 oraciones\n",
            "Episodio 197: 1323 oraciones\n",
            "Episodio 198: 1319 oraciones\n",
            "Episodio 199: 1999 oraciones\n",
            "Episodio 200: 1792 oraciones\n",
            "Episodio 201: 1447 oraciones\n",
            "Episodio 202: 1367 oraciones\n",
            "Episodio 203: 1174 oraciones\n",
            "Episodio 204: 1646 oraciones\n",
            "Episodio 205: 1287 oraciones\n",
            "Episodio 206: 1663 oraciones\n",
            "Episodio 208: 2042 oraciones\n",
            "Episodio 209: 880 oraciones\n",
            "Episodio 210: 1552 oraciones\n",
            "Episodio 211: 1025 oraciones\n",
            "Episodio 212: 1930 oraciones\n",
            "Episodio 213: 1676 oraciones\n",
            "Episodio 214: 1033 oraciones\n",
            "Episodio 215: 1568 oraciones\n",
            "Episodio 216: 2620 oraciones\n",
            "Episodio 217: 1583 oraciones\n",
            "Episodio 218: 1140 oraciones\n",
            "Episodio 219: 1591 oraciones\n",
            "Episodio 220: 2029 oraciones\n",
            "Episodio 221: 1319 oraciones\n",
            "Episodio 222: 992 oraciones\n",
            "Episodio 223: 3198 oraciones\n",
            "Episodio 224: 2754 oraciones\n",
            "Episodio 225: 1615 oraciones\n",
            "Episodio 226: 997 oraciones\n",
            "Episodio 227: 1598 oraciones\n",
            "Episodio 228: 1209 oraciones\n",
            "Episodio 229: 1217 oraciones\n",
            "Episodio 230: 3586 oraciones\n",
            "Episodio 231: 1708 oraciones\n",
            "Episodio 232: 869 oraciones\n",
            "Episodio 233: 770 oraciones\n",
            "Episodio 234: 2274 oraciones\n",
            "Episodio 235: 1155 oraciones\n",
            "Episodio 236: 1796 oraciones\n",
            "Episodio 237: 1842 oraciones\n",
            "Episodio 238: 871 oraciones\n",
            "Episodio 239: 1357 oraciones\n",
            "Episodio 240: 1290 oraciones\n",
            "Episodio 241: 1868 oraciones\n",
            "Episodio 242: 2272 oraciones\n",
            "Episodio 243: 1983 oraciones\n",
            "Episodio 244: 1607 oraciones\n",
            "Episodio 245: 770 oraciones\n",
            "Episodio 246: 959 oraciones\n",
            "Episodio 247: 2655 oraciones\n",
            "Episodio 248: 1242 oraciones\n",
            "Episodio 249: 723 oraciones\n",
            "Episodio 250: 1838 oraciones\n",
            "Episodio 251: 711 oraciones\n",
            "Episodio 252: 1538 oraciones\n",
            "Episodio 253: 3039 oraciones\n",
            "Episodio 254: 1362 oraciones\n",
            "Episodio 255: 2000 oraciones\n",
            "Episodio 256: 1682 oraciones\n",
            "Episodio 257: 3244 oraciones\n",
            "Episodio 258: 1662 oraciones\n",
            "Episodio 259: 1244 oraciones\n",
            "Episodio 260: 731 oraciones\n",
            "Episodio 261: 1542 oraciones\n",
            "Episodio 262: 1007 oraciones\n",
            "Episodio 263: 1205 oraciones\n",
            "Episodio 264: 2156 oraciones\n",
            "Episodio 265: 1891 oraciones\n",
            "Episodio 266: 1192 oraciones\n",
            "Episodio 267: 1064 oraciones\n",
            "Episodio 269: 2869 oraciones\n",
            "Episodio 270: 1352 oraciones\n",
            "Episodio 271: 1143 oraciones\n",
            "Episodio 272: 3654 oraciones\n",
            "Episodio 273: 1724 oraciones\n",
            "Episodio 274: 1849 oraciones\n",
            "Episodio 275: 325 oraciones\n",
            "Episodio 276: 2354 oraciones\n",
            "Episodio 277: 2638 oraciones\n",
            "Episodio 278: 1589 oraciones\n",
            "Episodio 279: 3189 oraciones\n",
            "Episodio 280: 803 oraciones\n",
            "Episodio 281: 1588 oraciones\n",
            "Episodio 284: 2730 oraciones\n",
            "Episodio 285: 1834 oraciones\n",
            "Episodio 286: 1186 oraciones\n",
            "Episodio 288: 3298 oraciones\n",
            "Episodio 289: 1319 oraciones\n",
            "Episodio 290: 1584 oraciones\n",
            "Episodio 292: 2927 oraciones\n",
            "Episodio 293: 2020 oraciones\n",
            "Episodio 294: 2100 oraciones\n",
            "Episodio 295: 1876 oraciones\n",
            "Episodio 296: 1637 oraciones\n",
            "Episodio 297: 1599 oraciones\n",
            "Episodio 298: 1359 oraciones\n",
            "Episodio 299: 1276 oraciones\n",
            "Episodio 300: 1523 oraciones\n",
            "Episodio 301: 2229 oraciones\n",
            "Episodio 302: 1422 oraciones\n",
            "Episodio 303: 2721 oraciones\n",
            "Episodio 304: 1534 oraciones\n",
            "Episodio 305: 1163 oraciones\n",
            "Episodio 306: 1210 oraciones\n",
            "Episodio 307: 1782 oraciones\n",
            "Episodio 308: 1701 oraciones\n",
            "Episodio 309: 2939 oraciones\n",
            "Episodio 310: 2558 oraciones\n",
            "Episodio 311: 2432 oraciones\n",
            "Episodio 312: 2386 oraciones\n",
            "Episodio 313: 2695 oraciones\n",
            "Episodio 314: 2531 oraciones\n",
            "Episodio 315: 1793 oraciones\n",
            "Episodio 316: 445 oraciones\n",
            "Episodio 317: 1851 oraciones\n",
            "Episodio 318: 2455 oraciones\n",
            "Episodio 319: 2067 oraciones\n",
            "Episodio 320: 1156 oraciones\n",
            "Episodio 321: 964 oraciones\n",
            "Episodio 322: 2000 oraciones\n",
            "Episodio 323: 2081 oraciones\n",
            "Episodio 324: 2143 oraciones\n",
            "Episodio 325: 2096 oraciones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación de una Tabla de Oraciones\n",
        "\n",
        "1. **Creación de la Tabla de Oraciones**: Divide el texto de cada episodio en oraciones y crea un DataFrame con tres columnas: `ep_id` (ID del episodio), `st_id` (ID de la oración dentro del episodio), y `text` (la oración en sí).\n",
        "2. **Concatenación de Datos**: Utiliza `pd.concat()` para concatenar las oraciones de cada episodio al DataFrame final `sentences_df`.\n"
      ],
      "metadata": {
        "id": "-nw-AYWvHE34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Crear un DataFrame vacío para almacenar los resultados\n",
        "sentences_df = pd.DataFrame(columns=['ep_id', 'st_id', 'text'])\n",
        "\n",
        "# Iterar sobre cada fila del DataFrame original\n",
        "for index, row in data.iterrows():\n",
        "    # Obtener el ID del episodio\n",
        "    ep_id = row['id']\n",
        "\n",
        "    # Dividir el texto en oraciones utilizando el delimitador de oraciones\n",
        "    sentences = [s.strip() for s in re.split(r'[.!?]+', str(row['text']).strip()) if s.strip()]\n",
        "\n",
        "    # Crear un DataFrame temporal para las oraciones del episodio\n",
        "    temp_df = pd.DataFrame({\n",
        "        'ep_id': ep_id,\n",
        "        'st_id': range(1, len(sentences) + 1),\n",
        "        'text': sentences\n",
        "    })\n",
        "\n",
        "    # Concatenar con el DataFrame final\n",
        "    sentences_df = pd.concat([sentences_df, temp_df], ignore_index=True)\n",
        "\n",
        "# Mostrar los primeros registros de la tabla creada\n",
        "print(sentences_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KZlDdxWACMg",
        "outputId": "e9203e3b-6b60-4139-b2e1-26a0d4e549e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       ep_id st_id                                               text\n",
            "0          1     1  As part of MIT course 6S099, Artificial Genera...\n",
            "1          1     2                      He is a professor here at MIT\n",
            "2          1     3  He's a physicist, spent a large part of his ca...\n",
            "3          1     4  But he's also studied and delved into the bene...\n",
            "4          1     5  Amongst many other things, he is the cofounder...\n",
            "...      ...   ...                                                ...\n",
            "447428   325  2093                                 Is it in the cells\n",
            "447429   325  2094  There are many, many layers to this as always ...\n",
            "447430   325  2095                     So there are chemical networks\n",
            "447431   325  2096    So for example, gene regulatory networks, right\n",
            "447432   325  2097   Which, or basically any kind of chemical pathway\n",
            "\n",
            "[447433 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1T9vUfBCjej",
        "outputId": "3be58a09-3134-406a-e12c-f544ee68a4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cálculo de Embeddings para las Oraciones\n",
        "\n",
        "1. **Instalación de `sentence-transformers`**: Utiliza `pip` para instalar el paquete necesario para generar embeddings de oraciones.\n",
        "2. **Carga del Modelo Preentrenado**: Se carga el modelo `all-MiniLM-L6-v2` de `sentence-transformers`, que es adecuado para generar embeddings de texto de tamaño pequeño y rápido.\n",
        "3. **Generación de Embeddings**: Divide el texto en oraciones y calcula los embeddings en lotes (por eficiencia). Los embeddings generados se agregan como una nueva columna en `sentences_df`.\n",
        "4. **Mostrar Resultados**: Muestra las primeras filas del DataFrame con los embeddings generados.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DCwzsLJFHM_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Cargar el modelo preentrenado para embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Crear una lista para almacenar los datos temporalmente\n",
        "data_list = []\n",
        "\n",
        "# Iterar sobre cada fila del DataFrame original con tqdm para mostrar el progreso\n",
        "for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Procesando oraciones\"):\n",
        "    # Obtener el ID del episodio\n",
        "    ep_id = row['id']\n",
        "\n",
        "    # Dividir el texto en oraciones\n",
        "    sentences = [s.strip() for s in re.split(r'[.!?]+', str(row['text']).strip()) if s.strip()]\n",
        "\n",
        "    # Agregar las oraciones con IDs a la lista\n",
        "    data_list.extend([{'ep_id': ep_id, 'st_id': idx + 1, 'text': sentence}\n",
        "                      for idx, sentence in enumerate(sentences)])\n",
        "\n",
        "# Crear un DataFrame final a partir de la lista\n",
        "sentences_df = pd.DataFrame(data_list)\n",
        "\n",
        "# Calcular los embeddings por lotes con tqdm para mostrar el progreso\n",
        "batch_size = 64  # Ajusta el tamaño del lote según la memoria disponible\n",
        "embeddings = []\n",
        "\n",
        "for i in tqdm(range(0, len(sentences_df), batch_size), desc=\"Calculando embeddings\"):\n",
        "    batch = sentences_df['text'][i:i+batch_size].tolist()\n",
        "    embeddings.extend(model.encode(batch))\n",
        "\n",
        "# Agregar los embeddings al DataFrame\n",
        "sentences_df['embedding'] = embeddings\n",
        "\n",
        "# Mostrar las primeras filas con embeddings\n",
        "print(sentences_df.head())\n"
      ],
      "metadata": {
        "id": "9-_o6gzLACSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75522e90-ab2f-4a7e-d68b-280757a6d580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando oraciones: 100%|██████████| 319/319 [00:02<00:00, 111.57it/s]\n",
            "Calculando embeddings: 100%|██████████| 6992/6992 [03:54<00:00, 29.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ep_id  st_id                                               text  \\\n",
            "0      1      1  As part of MIT course 6S099, Artificial Genera...   \n",
            "1      1      2                      He is a professor here at MIT   \n",
            "2      1      3  He's a physicist, spent a large part of his ca...   \n",
            "3      1      4  But he's also studied and delved into the bene...   \n",
            "4      1      5  Amongst many other things, he is the cofounder...   \n",
            "\n",
            "                                           embedding  \n",
            "0  [-0.09496674, -0.09888684, 0.022210749, -0.051...  \n",
            "1  [-0.07171491, -0.01610179, 0.026951278, -0.035...  \n",
            "2  [0.011199181, -0.0019410783, 0.0041966145, 0.0...  \n",
            "3  [0.031063225, 0.0071431664, 0.0050430056, -0.0...  \n",
            "4  [-0.05261418, -0.0066042743, -0.039290454, -0....  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clusterización de Oraciones y Capítulos\n",
        "\n",
        "1. **Clusterización de Oraciones:**\n",
        "   - **Método:** `KMeans` con un máximo de 10 clusters dinámicos por capítulo.\n",
        "   - **Resultado:** Cada oración recibe un identificador de cluster (`cluster`).\n",
        "\n",
        "2. **Vector Representativo del Capítulo:**\n",
        "   - **Método:** Media de los embeddings de las oraciones.\n",
        "   - **Resultado:** Cada capítulo tiene un vector que resume su contenido.\n",
        "\n",
        "3. **Clusterización de Capítulos:**\n",
        "   - **Método:** `MiniBatchKMeans` para agrupar capítulos en 20 clusters (configurable).\n",
        "   - **Resultado:** Cada capítulo recibe un identificador de cluster (`episode_cluster`).\n",
        "\n",
        "4. **Evaluación:**\n",
        "   - **Métrica:** `Silhouette Score` para medir la calidad de los clusters.\n",
        "\n",
        "5. **Resultados Finales:**\n",
        "   - **Archivo:** `clustered_sentences.csv` con:\n",
        "     - `cluster`: Cluster de cada oración.\n",
        "     - `episode_cluster`: Cluster asignado al capítulo.\n"
      ],
      "metadata": {
        "id": "YtabubCuFVFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paso 1: Clusterización de oraciones por capítulo\n",
        "clusters_by_episode = {}\n",
        "\n",
        "for ep_id in tqdm(sentences_df['ep_id'].unique(), desc=\"Clusterizando oraciones por capítulo\"):\n",
        "    # Filtrar las oraciones del capítulo actual\n",
        "    chapter_data = sentences_df[sentences_df['ep_id'] == ep_id]\n",
        "    embeddings = np.vstack(chapter_data['embedding'].to_numpy())  # Convertir embeddings a matriz numpy\n",
        "\n",
        "    # Calcular un número de clusters dinámico (máximo 10 o menos si hay pocas oraciones)\n",
        "    max_clusters = min(10, len(embeddings))  # Ajusta según tus datos\n",
        "    if max_clusters > 1:\n",
        "        kmeans = KMeans(n_clusters=max_clusters, random_state=42, n_init=10)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "    else:\n",
        "        labels = np.zeros(len(embeddings))  # Todo un solo cluster si hay pocas oraciones\n",
        "\n",
        "    # Guardar resultados en el DataFrame\n",
        "    sentences_df.loc[chapter_data.index, 'cluster'] = labels\n",
        "    clusters_by_episode[ep_id] = labels\n",
        "\n",
        "# Paso 2: Vector representativo de cada capítulo\n",
        "episode_vectors = sentences_df.groupby('ep_id')['embedding'].apply(\n",
        "    lambda x: np.mean(np.vstack(x), axis=0)\n",
        ").to_numpy()\n",
        "\n",
        "# Paso 3: Clusterización de capítulos\n",
        "n_episode_clusters = 20  # Ajusta el número de clusters según los datos\n",
        "episode_kmeans = MiniBatchKMeans(n_clusters=n_episode_clusters, random_state=42, batch_size=100)\n",
        "episode_labels = episode_kmeans.fit_predict(np.vstack(episode_vectors))\n",
        "\n",
        "# Asignar etiquetas de clusters a capítulos\n",
        "episode_cluster_mapping = dict(zip(sentences_df['ep_id'].unique(), episode_labels))\n",
        "sentences_df['episode_cluster'] = sentences_df['ep_id'].map(episode_cluster_mapping)\n",
        "\n",
        "# Paso 4: Resultados\n",
        "for ep_id, cluster_id in episode_cluster_mapping.items():\n",
        "    print(f\"Capítulo {ep_id} pertenece al cluster {cluster_id}\")\n"
      ],
      "metadata": {
        "id": "uqAOZxM0EK5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f52e391a-3500-4eb7-b3f9-9cef8d8bd24c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clusterizando oraciones por capítulo: 100%|██████████| 318/318 [02:58<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capítulo 1 pertenece al cluster 1\n",
            "Capítulo 2 pertenece al cluster 1\n",
            "Capítulo 3 pertenece al cluster 16\n",
            "Capítulo 4 pertenece al cluster 15\n",
            "Capítulo 5 pertenece al cluster 15\n",
            "Capítulo 6 pertenece al cluster 12\n",
            "Capítulo 7 pertenece al cluster 12\n",
            "Capítulo 8 pertenece al cluster 12\n",
            "Capítulo 9 pertenece al cluster 16\n",
            "Capítulo 10 pertenece al cluster 15\n",
            "Capítulo 11 pertenece al cluster 15\n",
            "Capítulo 12 pertenece al cluster 16\n",
            "Capítulo 13 pertenece al cluster 15\n",
            "Capítulo 14 pertenece al cluster 7\n",
            "Capítulo 15 pertenece al cluster 14\n",
            "Capítulo 16 pertenece al cluster 8\n",
            "Capítulo 17 pertenece al cluster 16\n",
            "Capítulo 18 pertenece al cluster 9\n",
            "Capítulo 19 pertenece al cluster 15\n",
            "Capítulo 20 pertenece al cluster 16\n",
            "Capítulo 21 pertenece al cluster 12\n",
            "Capítulo 22 pertenece al cluster 12\n",
            "Capítulo 23 pertenece al cluster 12\n",
            "Capítulo 24 pertenece al cluster 8\n",
            "Capítulo 25 pertenece al cluster 1\n",
            "Capítulo 26 pertenece al cluster 1\n",
            "Capítulo 27 pertenece al cluster 17\n",
            "Capítulo 28 pertenece al cluster 9\n",
            "Capítulo 29 pertenece al cluster 12\n",
            "Capítulo 30 pertenece al cluster 12\n",
            "Capítulo 31 pertenece al cluster 9\n",
            "Capítulo 32 pertenece al cluster 1\n",
            "Capítulo 33 pertenece al cluster 9\n",
            "Capítulo 34 pertenece al cluster 3\n",
            "Capítulo 35 pertenece al cluster 12\n",
            "Capítulo 36 pertenece al cluster 15\n",
            "Capítulo 37 pertenece al cluster 9\n",
            "Capítulo 38 pertenece al cluster 15\n",
            "Capítulo 39 pertenece al cluster 9\n",
            "Capítulo 40 pertenece al cluster 15\n",
            "Capítulo 41 pertenece al cluster 1\n",
            "Capítulo 42 pertenece al cluster 12\n",
            "Capítulo 43 pertenece al cluster 15\n",
            "Capítulo 44 pertenece al cluster 15\n",
            "Capítulo 45 pertenece al cluster 18\n",
            "Capítulo 46 pertenece al cluster 3\n",
            "Capítulo 47 pertenece al cluster 1\n",
            "Capítulo 48 pertenece al cluster 12\n",
            "Capítulo 49 pertenece al cluster 1\n",
            "Capítulo 50 pertenece al cluster 15\n",
            "Capítulo 51 pertenece al cluster 18\n",
            "Capítulo 52 pertenece al cluster 12\n",
            "Capítulo 53 pertenece al cluster 1\n",
            "Capítulo 54 pertenece al cluster 8\n",
            "Capítulo 55 pertenece al cluster 2\n",
            "Capítulo 56 pertenece al cluster 1\n",
            "Capítulo 57 pertenece al cluster 15\n",
            "Capítulo 58 pertenece al cluster 1\n",
            "Capítulo 59 pertenece al cluster 9\n",
            "Capítulo 60 pertenece al cluster 1\n",
            "Capítulo 61 pertenece al cluster 15\n",
            "Capítulo 62 pertenece al cluster 12\n",
            "Capítulo 63 pertenece al cluster 4\n",
            "Capítulo 64 pertenece al cluster 1\n",
            "Capítulo 65 pertenece al cluster 15\n",
            "Capítulo 66 pertenece al cluster 8\n",
            "Capítulo 67 pertenece al cluster 11\n",
            "Capítulo 68 pertenece al cluster 8\n",
            "Capítulo 69 pertenece al cluster 1\n",
            "Capítulo 70 pertenece al cluster 12\n",
            "Capítulo 71 pertenece al cluster 15\n",
            "Capítulo 72 pertenece al cluster 15\n",
            "Capítulo 73 pertenece al cluster 12\n",
            "Capítulo 74 pertenece al cluster 8\n",
            "Capítulo 75 pertenece al cluster 15\n",
            "Capítulo 76 pertenece al cluster 1\n",
            "Capítulo 77 pertenece al cluster 1\n",
            "Capítulo 78 pertenece al cluster 18\n",
            "Capítulo 79 pertenece al cluster 1\n",
            "Capítulo 80 pertenece al cluster 17\n",
            "Capítulo 81 pertenece al cluster 14\n",
            "Capítulo 82 pertenece al cluster 2\n",
            "Capítulo 83 pertenece al cluster 1\n",
            "Capítulo 84 pertenece al cluster 8\n",
            "Capítulo 85 pertenece al cluster 1\n",
            "Capítulo 86 pertenece al cluster 16\n",
            "Capítulo 87 pertenece al cluster 1\n",
            "Capítulo 88 pertenece al cluster 3\n",
            "Capítulo 89 pertenece al cluster 15\n",
            "Capítulo 90 pertenece al cluster 19\n",
            "Capítulo 91 pertenece al cluster 8\n",
            "Capítulo 92 pertenece al cluster 13\n",
            "Capítulo 93 pertenece al cluster 19\n",
            "Capítulo 94 pertenece al cluster 15\n",
            "Capítulo 95 pertenece al cluster 15\n",
            "Capítulo 96 pertenece al cluster 8\n",
            "Capítulo 97 pertenece al cluster 9\n",
            "Capítulo 98 pertenece al cluster 8\n",
            "Capítulo 99 pertenece al cluster 1\n",
            "Capítulo 101 pertenece al cluster 1\n",
            "Capítulo 102 pertenece al cluster 2\n",
            "Capítulo 103 pertenece al cluster 15\n",
            "Capítulo 104 pertenece al cluster 12\n",
            "Capítulo 105 pertenece al cluster 19\n",
            "Capítulo 106 pertenece al cluster 15\n",
            "Capítulo 107 pertenece al cluster 8\n",
            "Capítulo 108 pertenece al cluster 15\n",
            "Capítulo 109 pertenece al cluster 12\n",
            "Capítulo 110 pertenece al cluster 15\n",
            "Capítulo 111 pertenece al cluster 15\n",
            "Capítulo 112 pertenece al cluster 13\n",
            "Capítulo 113 pertenece al cluster 19\n",
            "Capítulo 114 pertenece al cluster 9\n",
            "Capítulo 115 pertenece al cluster 15\n",
            "Capítulo 116 pertenece al cluster 18\n",
            "Capítulo 117 pertenece al cluster 2\n",
            "Capítulo 118 pertenece al cluster 8\n",
            "Capítulo 119 pertenece al cluster 1\n",
            "Capítulo 120 pertenece al cluster 15\n",
            "Capítulo 121 pertenece al cluster 3\n",
            "Capítulo 122 pertenece al cluster 3\n",
            "Capítulo 123 pertenece al cluster 2\n",
            "Capítulo 124 pertenece al cluster 1\n",
            "Capítulo 125 pertenece al cluster 5\n",
            "Capítulo 126 pertenece al cluster 12\n",
            "Capítulo 127 pertenece al cluster 5\n",
            "Capítulo 128 pertenece al cluster 6\n",
            "Capítulo 129 pertenece al cluster 1\n",
            "Capítulo 130 pertenece al cluster 15\n",
            "Capítulo 131 pertenece al cluster 12\n",
            "Capítulo 132 pertenece al cluster 3\n",
            "Capítulo 133 pertenece al cluster 19\n",
            "Capítulo 134 pertenece al cluster 6\n",
            "Capítulo 135 pertenece al cluster 3\n",
            "Capítulo 136 pertenece al cluster 4\n",
            "Capítulo 137 pertenece al cluster 18\n",
            "Capítulo 138 pertenece al cluster 2\n",
            "Capítulo 139 pertenece al cluster 1\n",
            "Capítulo 140 pertenece al cluster 2\n",
            "Capítulo 141 pertenece al cluster 8\n",
            "Capítulo 142 pertenece al cluster 2\n",
            "Capítulo 143 pertenece al cluster 5\n",
            "Capítulo 144 pertenece al cluster 3\n",
            "Capítulo 145 pertenece al cluster 8\n",
            "Capítulo 146 pertenece al cluster 19\n",
            "Capítulo 147 pertenece al cluster 9\n",
            "Capítulo 148 pertenece al cluster 3\n",
            "Capítulo 149 pertenece al cluster 2\n",
            "Capítulo 150 pertenece al cluster 6\n",
            "Capítulo 151 pertenece al cluster 3\n",
            "Capítulo 152 pertenece al cluster 5\n",
            "Capítulo 153 pertenece al cluster 15\n",
            "Capítulo 154 pertenece al cluster 18\n",
            "Capítulo 155 pertenece al cluster 16\n",
            "Capítulo 156 pertenece al cluster 6\n",
            "Capítulo 157 pertenece al cluster 18\n",
            "Capítulo 158 pertenece al cluster 2\n",
            "Capítulo 159 pertenece al cluster 17\n",
            "Capítulo 160 pertenece al cluster 12\n",
            "Capítulo 161 pertenece al cluster 3\n",
            "Capítulo 162 pertenece al cluster 12\n",
            "Capítulo 163 pertenece al cluster 6\n",
            "Capítulo 164 pertenece al cluster 2\n",
            "Capítulo 165 pertenece al cluster 5\n",
            "Capítulo 166 pertenece al cluster 8\n",
            "Capítulo 167 pertenece al cluster 6\n",
            "Capítulo 168 pertenece al cluster 17\n",
            "Capítulo 169 pertenece al cluster 5\n",
            "Capítulo 170 pertenece al cluster 6\n",
            "Capítulo 171 pertenece al cluster 17\n",
            "Capítulo 172 pertenece al cluster 3\n",
            "Capítulo 173 pertenece al cluster 17\n",
            "Capítulo 174 pertenece al cluster 6\n",
            "Capítulo 175 pertenece al cluster 6\n",
            "Capítulo 176 pertenece al cluster 11\n",
            "Capítulo 177 pertenece al cluster 15\n",
            "Capítulo 178 pertenece al cluster 6\n",
            "Capítulo 179 pertenece al cluster 5\n",
            "Capítulo 180 pertenece al cluster 4\n",
            "Capítulo 181 pertenece al cluster 17\n",
            "Capítulo 182 pertenece al cluster 5\n",
            "Capítulo 183 pertenece al cluster 8\n",
            "Capítulo 184 pertenece al cluster 18\n",
            "Capítulo 185 pertenece al cluster 2\n",
            "Capítulo 186 pertenece al cluster 8\n",
            "Capítulo 187 pertenece al cluster 1\n",
            "Capítulo 188 pertenece al cluster 17\n",
            "Capítulo 189 pertenece al cluster 19\n",
            "Capítulo 190 pertenece al cluster 1\n",
            "Capítulo 191 pertenece al cluster 8\n",
            "Capítulo 192 pertenece al cluster 17\n",
            "Capítulo 193 pertenece al cluster 8\n",
            "Capítulo 194 pertenece al cluster 19\n",
            "Capítulo 195 pertenece al cluster 18\n",
            "Capítulo 196 pertenece al cluster 6\n",
            "Capítulo 197 pertenece al cluster 5\n",
            "Capítulo 198 pertenece al cluster 1\n",
            "Capítulo 199 pertenece al cluster 5\n",
            "Capítulo 200 pertenece al cluster 6\n",
            "Capítulo 201 pertenece al cluster 18\n",
            "Capítulo 202 pertenece al cluster 19\n",
            "Capítulo 203 pertenece al cluster 10\n",
            "Capítulo 204 pertenece al cluster 1\n",
            "Capítulo 205 pertenece al cluster 5\n",
            "Capítulo 206 pertenece al cluster 15\n",
            "Capítulo 208 pertenece al cluster 1\n",
            "Capítulo 209 pertenece al cluster 12\n",
            "Capítulo 210 pertenece al cluster 2\n",
            "Capítulo 211 pertenece al cluster 1\n",
            "Capítulo 212 pertenece al cluster 1\n",
            "Capítulo 213 pertenece al cluster 18\n",
            "Capítulo 214 pertenece al cluster 1\n",
            "Capítulo 215 pertenece al cluster 15\n",
            "Capítulo 216 pertenece al cluster 19\n",
            "Capítulo 217 pertenece al cluster 3\n",
            "Capítulo 218 pertenece al cluster 8\n",
            "Capítulo 219 pertenece al cluster 3\n",
            "Capítulo 220 pertenece al cluster 5\n",
            "Capítulo 221 pertenece al cluster 15\n",
            "Capítulo 222 pertenece al cluster 1\n",
            "Capítulo 223 pertenece al cluster 5\n",
            "Capítulo 224 pertenece al cluster 12\n",
            "Capítulo 225 pertenece al cluster 1\n",
            "Capítulo 226 pertenece al cluster 12\n",
            "Capítulo 227 pertenece al cluster 2\n",
            "Capítulo 228 pertenece al cluster 5\n",
            "Capítulo 229 pertenece al cluster 0\n",
            "Capítulo 230 pertenece al cluster 5\n",
            "Capítulo 231 pertenece al cluster 17\n",
            "Capítulo 232 pertenece al cluster 1\n",
            "Capítulo 233 pertenece al cluster 2\n",
            "Capítulo 234 pertenece al cluster 1\n",
            "Capítulo 235 pertenece al cluster 19\n",
            "Capítulo 236 pertenece al cluster 5\n",
            "Capítulo 237 pertenece al cluster 9\n",
            "Capítulo 238 pertenece al cluster 19\n",
            "Capítulo 239 pertenece al cluster 4\n",
            "Capítulo 240 pertenece al cluster 8\n",
            "Capítulo 241 pertenece al cluster 9\n",
            "Capítulo 242 pertenece al cluster 5\n",
            "Capítulo 243 pertenece al cluster 3\n",
            "Capítulo 244 pertenece al cluster 4\n",
            "Capítulo 245 pertenece al cluster 5\n",
            "Capítulo 246 pertenece al cluster 1\n",
            "Capítulo 247 pertenece al cluster 6\n",
            "Capítulo 248 pertenece al cluster 4\n",
            "Capítulo 249 pertenece al cluster 19\n",
            "Capítulo 250 pertenece al cluster 8\n",
            "Capítulo 251 pertenece al cluster 4\n",
            "Capítulo 252 pertenece al cluster 8\n",
            "Capítulo 253 pertenece al cluster 5\n",
            "Capítulo 254 pertenece al cluster 19\n",
            "Capítulo 255 pertenece al cluster 5\n",
            "Capítulo 256 pertenece al cluster 4\n",
            "Capítulo 257 pertenece al cluster 18\n",
            "Capítulo 258 pertenece al cluster 15\n",
            "Capítulo 259 pertenece al cluster 3\n",
            "Capítulo 260 pertenece al cluster 5\n",
            "Capítulo 261 pertenece al cluster 1\n",
            "Capítulo 262 pertenece al cluster 8\n",
            "Capítulo 263 pertenece al cluster 19\n",
            "Capítulo 264 pertenece al cluster 2\n",
            "Capítulo 265 pertenece al cluster 5\n",
            "Capítulo 266 pertenece al cluster 17\n",
            "Capítulo 267 pertenece al cluster 8\n",
            "Capítulo 269 pertenece al cluster 1\n",
            "Capítulo 270 pertenece al cluster 2\n",
            "Capítulo 271 pertenece al cluster 18\n",
            "Capítulo 272 pertenece al cluster 5\n",
            "Capítulo 273 pertenece al cluster 4\n",
            "Capítulo 274 pertenece al cluster 2\n",
            "Capítulo 275 pertenece al cluster 2\n",
            "Capítulo 276 pertenece al cluster 11\n",
            "Capítulo 277 pertenece al cluster 2\n",
            "Capítulo 278 pertenece al cluster 2\n",
            "Capítulo 279 pertenece al cluster 1\n",
            "Capítulo 280 pertenece al cluster 9\n",
            "Capítulo 281 pertenece al cluster 2\n",
            "Capítulo 284 pertenece al cluster 11\n",
            "Capítulo 285 pertenece al cluster 6\n",
            "Capítulo 286 pertenece al cluster 4\n",
            "Capítulo 288 pertenece al cluster 5\n",
            "Capítulo 289 pertenece al cluster 4\n",
            "Capítulo 290 pertenece al cluster 2\n",
            "Capítulo 292 pertenece al cluster 8\n",
            "Capítulo 293 pertenece al cluster 1\n",
            "Capítulo 294 pertenece al cluster 3\n",
            "Capítulo 295 pertenece al cluster 4\n",
            "Capítulo 296 pertenece al cluster 6\n",
            "Capítulo 297 pertenece al cluster 19\n",
            "Capítulo 298 pertenece al cluster 2\n",
            "Capítulo 299 pertenece al cluster 16\n",
            "Capítulo 300 pertenece al cluster 5\n",
            "Capítulo 301 pertenece al cluster 3\n",
            "Capítulo 302 pertenece al cluster 1\n",
            "Capítulo 303 pertenece al cluster 11\n",
            "Capítulo 304 pertenece al cluster 2\n",
            "Capítulo 305 pertenece al cluster 18\n",
            "Capítulo 306 pertenece al cluster 15\n",
            "Capítulo 307 pertenece al cluster 17\n",
            "Capítulo 308 pertenece al cluster 9\n",
            "Capítulo 309 pertenece al cluster 12\n",
            "Capítulo 310 pertenece al cluster 6\n",
            "Capítulo 311 pertenece al cluster 6\n",
            "Capítulo 312 pertenece al cluster 2\n",
            "Capítulo 313 pertenece al cluster 2\n",
            "Capítulo 314 pertenece al cluster 3\n",
            "Capítulo 315 pertenece al cluster 5\n",
            "Capítulo 316 pertenece al cluster 4\n",
            "Capítulo 317 pertenece al cluster 1\n",
            "Capítulo 318 pertenece al cluster 1\n",
            "Capítulo 319 pertenece al cluster 5\n",
            "Capítulo 320 pertenece al cluster 4\n",
            "Capítulo 321 pertenece al cluster 8\n",
            "Capítulo 322 pertenece al cluster 3\n",
            "Capítulo 323 pertenece al cluster 5\n",
            "Capítulo 324 pertenece al cluster 5\n",
            "Capítulo 325 pertenece al cluster 1\n"
          ]
        }
      ]
    }
  ]
}